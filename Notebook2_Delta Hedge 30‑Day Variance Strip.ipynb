{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120ec1ea-a2e9-49df-b464-fc1a4cfac9bd",
   "metadata": {},
   "source": [
    "# Notebook 2, Delta Hedge 30-Day Variance Strip (SPX), t₀ = 2020-06-15 → T* = 2020-07-13 (28d)\n",
    "\n",
    "**Goal.**\n",
    "\n",
    "Show that a quotes-only delta-hedged VIX-style static strip tracks a 30-day variance-swap payoff over the same span.\n",
    "\n",
    "**Inputs.**\n",
    "\n",
    "* From Notebook 1: `01_variance_strike/strip_30d.csv`, `01_variance_strike/variance_strike_summary.csv`\n",
    "* Options panel (mids): `02_hedge/spx_option_quotes_2020-06-15_to_2020-07-17.csv`\n",
    "* SPX closes: `02_hedge/spx_close_2020-06-01_to_2020-08-15.csv`\n",
    "* Note: `STRIKE_DIVISOR = 1000`; r = q = 0, quotes-only replication with mids, daily hedging at closes, no transaction cost.\n",
    "\n",
    "**Methods.**\n",
    "\n",
    "1. **Build the tradable strip.** Load NB1 legs; drop audit B rows; apply NB1 interpolation weights (`w_near`, `w_next`) so leg weights are in 30 day variance units.\n",
    "2. **Daily valuation with strict coverage.** From quotes, keep only the two NB1 expiries. A day is valid only if every required `(expiry, strike, side)` has a finite mid. On valid days, mark\n",
    "   $V_t = \\sum_i \\text{weight}_i \\cdot \\text{mid}_{i,t}$ (puts use `mid_P`, calls use `mid_C`).\n",
    "3. **Quotes-only delta.** Estimate delta as the rolling OLS slope of $V$ on $S$ over the last $k=5$ days; hedge with yesterday’s slope $\\Delta^{\\text{hedge}}_t = \\widehat{\\beta}_{t-1}$.\n",
    "4. **Hedge P&L and cumulation.** $\\text{HPnL}_t = - \\Delta^{\\text{hedge}}_t (S_t - S_{t-1})$; $\\Delta V^{\\text{hedged}}_t = (V_t - V_{t-1}) + \\text{HPnL}_t$. Cumulate to $T^*$.\n",
    "5. **Comparator and stop rule.** Pick $T^*$ as the closest tradable date to $t_0 + 30$ calendar days. Compute realized variance on the same span\n",
    "   $RV = \\big(\\sum (\\Delta \\ln S)^2\\big)\\cdot \\dfrac{365}{\\text{calendar days}}$. Compare the strip’s hedged total to $RV - \\sigma^2_{30\\text{d}}$ ($\\sigma^2_{30\\text{d}}$ from NB1).\n",
    "\n",
    "**Outputs.**\n",
    "\n",
    "* `02_hedge/hedge_log.csv` — daily log: `date, S, V, dS, dV, delta_hedge, hedge_pnl, dV_hedged, V_unhedged_cum, V_hedged_cum`\n",
    "* `02_hedge/hedge_summary.csv` — one row: `t0, T_star, days, sigma2_30d_from_NB1, RV_30d, RV_minus_sigma2, strip_hedged_total, tracking_error`\n",
    "\n",
    "**Results (2020-06-15 → 2020-07-13).**\n",
    "\n",
    "* $\\sigma^2_{30\\text{d}}$ (NB1): **0.125807**\n",
    "* $RV_{30\\text{d}}$: **0.039284**\n",
    "* Target $RV - \\sigma^2_{30\\text{d}}$: **−0.086523**\n",
    "* Strip hedged total: **−0.131393**\n",
    "* Tracking error (hedged − target): **−0.04487**\n",
    "\n",
    "**Interpretation.**\n",
    "\n",
    "The quotes-only delta-hedged strip matches the 30-day variance-swap payoff in sign and order of magnitude. The residual is consistent with the finite strike grid, timing (mids vs. close), the non-tradable VIX boundary term $B(T)$ (excluded), and daily (not continuous) hedging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9b921",
   "metadata": {},
   "source": [
    "**Set up.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae84a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "STRIKE_DIVISOR = 1000\n",
    "\n",
    "DIR_VS = \"01_variance_strike\"\n",
    "DIR_H2 = \"02_hedge\"\n",
    "OPTS_CSV = r\"02_hedge/spx_option_quotes_2020-06-15_to_2020-07-17.csv\"\n",
    "CLOSES_CSV = r\"02_hedge/spx_close_2020-06-01_to_2020-08-15.csv\"\n",
    "\n",
    "# Notebook 1 outputs\n",
    "STRIP_30D_CSV = os.path.join(DIR_VS, \"strip_30d.csv\")\n",
    "VS_SUMMARY = os.path.join(DIR_VS, \"variance_strike_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672a6cd9-330f-4bae-b9ff-e5fcf8bb95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Notebook 1 strip and summary, apply interpolation weights\n",
    "strip = pd.read_csv(STRIP_30D_CSV)\n",
    "vs = pd.read_csv(VS_SUMMARY)\n",
    "\n",
    "strip[\"expiry\"] = pd.to_datetime(strip[\"expiry\"]).dt.date\n",
    "strip = strip[strip[\"side\"].isin([\"C\",\"P\"])].copy()\n",
    "expiries = sorted(strip[\"expiry\"].unique())\n",
    "exp_near, exp_next = expiries\n",
    "\n",
    "t0 = pd.to_datetime(vs.loc[0, \"t0\"]).date()\n",
    "sigma2_30d = float(vs.loc[0, \"sigma2_30d\"])\n",
    "w_near = float(vs.loc[0, \"w_near\"])\n",
    "w_next = float(vs.loc[0, \"w_next\"])\n",
    "\n",
    "# apply per-expiry interpolation weights\n",
    "strip[\"expiry_weight\"] = np.where(strip[\"expiry\"] == exp_near, w_near, w_next)\n",
    "strip[\"weight\"] = strip[\"weight\"] * strip[\"expiry_weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56a486",
   "metadata": {},
   "source": [
    "**Market data.** Load option mids and underlying closes; pivot to mid_C/mid_P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf1c83c-15ba-4cb7-ae6a-3fc463705e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option quotes\n",
    "opts = pd.read_csv(OPTS_CSV, low_memory=False)\n",
    "\n",
    "opts[\"date\"] = pd.to_datetime(opts[\"date\"]).dt.date\n",
    "opts[\"exdate\"] = pd.to_datetime(opts[\"exdate\"]).dt.date\n",
    "opts[\"K\"] = pd.to_numeric(opts[\"strike_price\"], errors=\"coerce\") / STRIKE_DIVISOR\n",
    "bb = pd.to_numeric(opts[\"best_bid\"], errors=\"coerce\")\n",
    "bo = pd.to_numeric(opts[\"best_offer\"], errors=\"coerce\")\n",
    "opts[\"mid\"] = (bb + bo) / 2.0\n",
    "opts = opts.dropna(subset=[\"date\", \"exdate\", \"K\", \"mid\", \"cp_flag\"])\n",
    "opts = opts[opts[\"exdate\"].isin(expiries)].copy()\n",
    "\n",
    "pvt = (opts\n",
    "       .pivot_table(index=[\"date\",\"exdate\",\"K\"], columns=\"cp_flag\", values=\"mid\", aggfunc=\"mean\")\n",
    "       .reset_index()\n",
    "       .rename(columns={\"C\":\"mid_C\",\"P\":\"mid_P\"}))\n",
    "\n",
    "# closes\n",
    "cl = pd.read_csv(CLOSES_CSV)\n",
    "cl[\"date\"] = pd.to_datetime(cl[\"date\"]).dt.date\n",
    "cl = cl.dropna(subset=[\"date\",\"close\"]).sort_values(\"date\")\n",
    "S_map = dict(zip(cl[\"date\"], pd.to_numeric(cl[\"close\"], errors=\"coerce\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148e9d2",
   "metadata": {},
   "source": [
    "**Marking.** Build $V_t$ only on days with a full cross‑section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca7a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build daily strip value V_t in variance units only on days with a strict full cross‑section\n",
    "strip_legs = strip[[\"expiry\",\"side\",\"strike\",\"weight\"]].copy()\n",
    "need_keys = strip_legs.rename(columns={\"strike\":\"K\",\"expiry\":\"exdate\"})[[\"exdate\",\"K\",\"side\"]]\n",
    "need_by_ex = {ex: need_keys[need_keys[\"exdate\"]==ex][[\"K\",\"side\"]].drop_duplicates() for ex in expiries}\n",
    "\n",
    "# strict full cross‑section\n",
    "def day_has_full_cross_section(pvt_day: pd.DataFrame) -> bool:\n",
    "    for ex, need in need_by_ex.items():\n",
    "        sub = pvt_day[pvt_day[\"exdate\"]==ex]\n",
    "        have_pairs = set()\n",
    "        for _, r in sub.iterrows():\n",
    "            if np.isfinite(r[\"mid_C\"]): have_pairs.add((round(float(r[\"K\"]),6), \"C\"))\n",
    "            if np.isfinite(r[\"mid_P\"]): have_pairs.add((round(float(r[\"K\"]),6), \"P\"))\n",
    "        req = set(zip(need[\"K\"].round(6), need[\"side\"]))\n",
    "        if not req.issubset(have_pairs):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def strip_value_variance_units(pvt_day: pd.DataFrame, strip_df: pd.DataFrame) -> float:\n",
    "    m = pvt_day.merge(strip_df.rename(columns={\"strike\":\"K\",\"expiry\":\"exdate\"}),\n",
    "                      on=[\"exdate\",\"K\"], how=\"inner\")\n",
    "    m[\"mid_px\"] = np.where(m[\"side\"]==\"C\", m[\"mid_C\"], m[\"mid_P\"])\n",
    "    return float((m[\"weight\"] * m[\"mid_px\"]).sum())\n",
    "\n",
    "dates = sorted(set(pvt[\"date\"].unique()) & set(S_map.keys()))\n",
    "rows = []\n",
    "for d in dates:\n",
    "    day = pvt[pvt[\"date\"]==d]\n",
    "    if not day_has_full_cross_section(day): \n",
    "        continue\n",
    "    V = strip_value_variance_units(day, strip_legs)\n",
    "    S = float(S_map[d])\n",
    "    rows.append({\"date\": d, \"S\": S, \"V\": V})\n",
    "\n",
    "path = pd.DataFrame(rows).sort_values(\"date\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125048a",
   "metadata": {},
   "source": [
    "**Hedge.** Rolling OLS delta from quotes $k=5$; cumulative hedged P&L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e0c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quotes-only rolling OLS delta (k=5)\n",
    "def ols_slope(y: np.ndarray, x: np.ndarray) -> float:\n",
    "    x = x.astype(float)\n",
    "    y = y.astype(float)\n",
    "    xm = x.mean()\n",
    "    ym = y.mean()\n",
    "    num = ((x - xm) * (y - ym)).sum()\n",
    "    den = ((x - xm)**2).sum()\n",
    "    return np.nan if den == 0 else float(num / den)\n",
    "\n",
    "path[\"dS\"] = path[\"S\"].diff()\n",
    "path[\"dV\"] = path[\"V\"].diff()\n",
    "\n",
    "k = 5\n",
    "path[\"delta_est\"] = np.nan\n",
    "for i in range(len(path)):\n",
    "    j0 = max(0, i - (k - 1))\n",
    "    window = path.iloc[j0:i+1]\n",
    "    if len(window) >= 3:\n",
    "        path.loc[path.index[i], \"delta_est\"] = ols_slope(window[\"V\"].values, window[\"S\"].values)\n",
    "\n",
    "path[\"delta_hedge\"] = path[\"delta_est\"].shift(1).fillna(0.0)\n",
    "# cumulative hedged P&L\n",
    "path[\"hedge_pnl\"] = - path[\"delta_hedge\"] * path[\"dS\"]\n",
    "path[\"dV_hedged\"] = path[\"dV\"] + path[\"hedge_pnl\"]\n",
    "path[\"V_unhedged_cum\"] = path[\"dV\"].fillna(0.0).cumsum()\n",
    "path[\"V_hedged_cum\"] = path[\"dV_hedged\"].fillna(0.0).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74458e86",
   "metadata": {},
   "source": [
    "**Finalize.** Stop at $T^* ≈ t_0+30d$, compute RV with exact span, write hedge_log and hedge_summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a421e1ff-b4e0-4a0d-bb2b-6c1f32635a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        t0     T_star  days  sigma2_30d_from_NB1   RV_30d  RV_minus_sigma2  strip_hedged_total  tracking_error\n",
      "2020-06-15 2020-07-13    28             0.125807 0.039284        -0.086523           -0.131393        -0.04487\n"
     ]
    }
   ],
   "source": [
    "# stop at T* ~ t0 + 30 days\n",
    "t_star_target = t0 + timedelta(days=30)\n",
    "path[\"abs_diff\"] = path[\"date\"].apply(lambda d: abs(d - t_star_target))\n",
    "t_star = path.loc[path[\"abs_diff\"].idxmin(), \"date\"]\n",
    "log_df = path[path[\"date\"] <= t_star].copy().drop(columns=[\"abs_diff\"])\n",
    "\n",
    "# compute realized variance on the exact same span\n",
    "cl_use = cl[(cl[\"date\"] >= log_df[\"date\"].min()) & (cl[\"date\"] <= t_star)].copy().sort_values(\"date\")\n",
    "days_calendar = (t_star - t0).days\n",
    "rv_30d = np.log(cl_use[\"close\"]).diff().pow(2).sum() * (365.0 / days_calendar)\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"t0\": t0,\n",
    "    \"T_star\": t_star,\n",
    "    \"days\": days_calendar,\n",
    "    \"sigma2_30d_from_NB1\": sigma2_30d,\n",
    "    \"RV_30d\": rv_30d,\n",
    "    \"RV_minus_sigma2\": rv_30d - sigma2_30d,\n",
    "    \"strip_hedged_total\": log_df[\"dV_hedged\"].sum(),\n",
    "    \"tracking_error\": log_df[\"dV_hedged\"].sum() - (rv_30d - sigma2_30d)\n",
    "}])\n",
    "\n",
    "os.makedirs(DIR_H2, exist_ok=True)\n",
    "log_fp = os.path.join(DIR_H2, \"hedge_log.csv\")\n",
    "sum_fp = os.path.join(DIR_H2, \"hedge_summary.csv\")\n",
    "log_cols = [\"date\", \"S\", \"V\", \"dS\", \"dV\", \"delta_hedge\", \"hedge_pnl\", \"dV_hedged\", \"V_unhedged_cum\", \"V_hedged_cum\"]\n",
    "log_df[log_cols].to_csv(log_fp, index=False)\n",
    "summary.to_csv(sum_fp, index=False)\n",
    "\n",
    "# outputs\n",
    "print(summary.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
